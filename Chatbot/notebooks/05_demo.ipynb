{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14266200,"sourceType":"datasetVersion","datasetId":9103675},{"sourceId":696122,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":527977,"modelId":542034}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================================\n# RAG CHATBOT TÆ¯ Váº¤N QUY CHáº¾ SINH VIÃŠN - KAGGLE NOTEBOOK\n# Káº¿ thá»«a tá»« há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡ LLM vá»›i behavior control\n# Platform: Kaggle (GPU T4 x2)\n# ================================================================\n\n# ================================================================\n# BÆ¯á»šC 1: CÃ€I Äáº¶T THÆ¯ VIá»†N\n# ================================================================\nprint(\"ğŸ”§ Äang cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t...\")\n\n!pip install -q langchain langchain-community langchain-huggingface chromadb gradio accelerate bitsandbytes transformers sentence-transformers\n\nprint(\"âœ… HoÃ n táº¥t cÃ i Ä‘áº·t thÆ° viá»‡n!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:31:11.199575Z","iopub.execute_input":"2026-02-11T08:31:11.200187Z","iopub.status.idle":"2026-02-11T08:31:35.346443Z","shell.execute_reply.started":"2026-02-11T08:31:11.200159Z","shell.execute_reply":"2026-02-11T08:31:35.345723Z"}},"outputs":[{"name":"stdout","text":"ğŸ”§ Äang cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t...\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m573.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nxmanager 0.7.1 requires sqlalchemy==1.2.19, but you have sqlalchemy 2.0.46 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mâœ… HoÃ n táº¥t cÃ i Ä‘áº·t thÆ° viá»‡n!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport gc\nimport torch\nimport re\nfrom typing import Dict, List, Tuple\nfrom datetime import datetime\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    pipeline\n)\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\nfrom langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import Document\nimport gradio as gr\n\nprint(\"âœ… Import thÆ° viá»‡n thÃ nh cÃ´ng!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:31:35.348291Z","iopub.execute_input":"2026-02-11T08:31:35.348614Z","iopub.status.idle":"2026-02-11T08:32:23.263865Z","shell.execute_reply.started":"2026-02-11T08:31:35.348584Z","shell.execute_reply":"2026-02-11T08:32:23.263150Z"}},"outputs":[{"name":"stderr","text":"2026-02-11 08:31:56.332735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770798716.722462      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770798716.838288      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770798717.755786      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770798717.755833      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770798717.755836      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770798717.755839      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"âœ… Import thÆ° viá»‡n thÃ nh cÃ´ng!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!cp -r /kaggle/input/chroma-db-for-hust-regulations/* /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:32:23.264735Z","iopub.execute_input":"2026-02-11T08:32:23.264986Z","iopub.status.idle":"2026-02-11T08:32:23.535939Z","shell.execute_reply.started":"2026-02-11T08:32:23.264962Z","shell.execute_reply":"2026-02-11T08:32:23.535141Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ================================================================\n# Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN (Thay Ä‘á»•i theo mÃ´i trÆ°á»ng cá»§a báº¡n)\n# ================================================================\nEMBEDDING_PATH = \"/kaggle/input/alibaba/transformers/default/1/embedding\"  # Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n nÃ y\nVECTOR_DB_PATH = \"/kaggle/working/chroma_db\"  # Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n nÃ y\nMODEL_ID = \"wanduc0701/hust_chatbot_qwen2.5_finetuned\"  # Model Ä‘Æ°á»£c fine-tune cho quy cháº¿ sinh viÃªn\n\nprint(f\"ğŸ“ Embedding Path: {EMBEDDING_PATH}\")\nprint(f\"ğŸ“ VectorDB Path: {VECTOR_DB_PATH}\")\nprint(f\"ğŸ¤– Model ID: {MODEL_ID}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:32:23.537232Z","iopub.execute_input":"2026-02-11T08:32:23.537541Z","iopub.status.idle":"2026-02-11T08:32:23.543109Z","shell.execute_reply.started":"2026-02-11T08:32:23.537509Z","shell.execute_reply":"2026-02-11T08:32:23.542319Z"}},"outputs":[{"name":"stdout","text":"ğŸ“ Embedding Path: /kaggle/input/alibaba/transformers/default/1/embedding\nğŸ“ VectorDB Path: /kaggle/working/chroma_db\nğŸ¤– Model ID: wanduc0701/hust_chatbot_qwen2.5_finetuned\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ================================================================\n# BÆ¯á»šC 2: THIáº¾T Káº¾ PROMPT TEMPLATE (CHUáº¨N FORMAT QWEN2.5)\n# ================================================================\n\nclass StudentRegulationPrompts:\n    @staticmethod\n    def create_qa_template() -> str:\n        template = \"\"\"<|im_start|>system\nBáº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn tÆ° váº¥n vá» quy cháº¿ sinh viÃªn cá»§a Äáº¡i há»c BÃ¡ch Khoa HÃ  Ná»™i.\nNHIá»†M Vá»¤ Cá»T LÃ•I: Chá»‰ tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p trong pháº§n CONTEXT.\n\nQUY Táº®C TUYá»†T Äá»I:\n1.  **CHá»ˆ Dá»°A VÃ€O CONTEXT**: Tuyá»‡t Ä‘á»‘i khÃ´ng sá»­ dá»¥ng kiáº¿n thá»©c bÃªn ngoÃ i. ToÃ n bá»™ cÃ¢u tráº£ lá»i pháº£i Ä‘Æ°á»£c rÃºt ra tá»« Context Ä‘Æ°á»£c cung cáº¥p.\n2.  **TRÃCH DáºªN NGUá»’N**: LuÃ´n báº¯t Ä‘áº§u cÃ¢u tráº£ lá»i báº±ng cÃ¡ch trÃ­ch dáº«n nguá»“n: \"Theo [Má»¥c/Äiá»u...] trong [TÃªn vÄƒn báº£n], thÃ¬...\".\n3.  **Xá»¬ LÃ KHI KHÃ”NG CÃ“ THÃ”NG TIN**: Náº¿u Context trá»‘ng hoáº·c khÃ´ng chá»©a thÃ´ng tin liÃªn quan Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i, báº¡n Báº®T BUá»˜C pháº£i tráº£ lá»i chÃ­nh xÃ¡c nhÆ° sau: \"Xin lá»—i, tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p khÃ´ng Ä‘á» cáº­p Ä‘áº¿n váº¥n Ä‘á» nÃ y.\"\n4.  **KHÃ”NG SUY DIá»„N**: KhÃ´ng Ä‘Æ°á»£c tá»± Ã½ suy diá»…n, giáº£ Ä‘á»‹nh hay bá»‹a Ä‘áº·t thÃ´ng tin khÃ´ng cÃ³ trong Context.\n<|im_end|>\n<|im_start|>user\nDÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c thÃ´ng tin trÃ­ch dáº«n tá»« vÄƒn báº£n quy cháº¿ (Context):\n---------------------\n{context}\n---------------------\n\nDá»±a vÃ o context trÃªn, hÃ£y tráº£ lá»i cÃ¢u há»i sau:\n{question}\n<|im_end|>\n<|im_start|>assistant\n\"\"\"\n        return template\n\nprint(\"âœ… ÄÃ£ cáº­p nháº­t Prompt chuáº©n format Qwen2.5!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:32:23.544728Z","iopub.execute_input":"2026-02-11T08:32:23.545026Z","iopub.status.idle":"2026-02-11T08:32:23.562787Z","shell.execute_reply.started":"2026-02-11T08:32:23.545002Z","shell.execute_reply":"2026-02-11T08:32:23.562162Z"}},"outputs":[{"name":"stdout","text":"âœ… ÄÃ£ cáº­p nháº­t Prompt chuáº©n format Qwen2.5!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ================================================================\n# BÆ¯á»šC 3: Táº¢I MODEL & TOKENIZER\n# ================================================================\nprint(\"\\nğŸš€ Äang táº£i LLM vá»›i quantization 4-bit...\")\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    low_cpu_mem_usage=True\n)\nmodel.eval()\n\ntext_gen_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=1024, # [CHá»ˆNH Sá»¬A] TÄƒng giá»›i háº¡n token Ä‘á»ƒ trÃ¡nh cÃ¢u tráº£ lá»i bá»‹ cáº¯t ngang\n    temperature=0.0,    # [CHá»ˆNH Sá»¬A] Äáº·t temperature=0.0 Ä‘á»ƒ cÃ¢u tráº£ lá»i nháº¥t quÃ¡n vÃ  bÃ¡m sÃ¡t context\n    top_p=0.9,\n    repetition_penalty=1.2,\n    do_sample=True,\n    return_full_text=False\n)\n\nllm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n\nprint(\"âœ… Khá»Ÿi táº¡o LLM Pipeline thÃ nh cÃ´ng!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:32:23.563512Z","iopub.execute_input":"2026-02-11T08:32:23.563811Z","iopub.status.idle":"2026-02-11T08:33:47.094926Z","shell.execute_reply.started":"2026-02-11T08:32:23.563779Z","shell.execute_reply":"2026-02-11T08:33:47.094307Z"}},"outputs":[{"name":"stdout","text":"\nğŸš€ Äang táº£i LLM vá»›i quantization 4-bit...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ca55939eb424cfda4c8ad6c7db36a9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ef53479617b49ea939335b4b920ad44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a81f8e6c239499a856068b698dfbc1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea3c8fbda01848c0b98bdba4ec694b38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06365b28b6fd4468ba1d89da5df4e7f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92f7b3c8836a43e7b6bb66d9c3ca9929"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cae9b63fd164f2190d063e39a9d483d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dbd11505a584107a320867fc20440c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3250dfb1ff7448439f373695bcc1e308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0efd439258c8489b8041c2c99c7ae39c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c2a3f1e1a3c433ba8429a6d564a92c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4b934b14c9b4576858b23070d2b2e28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c22bd50ec2234a71bcc2caeba53e46ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03ddc35015524dd483a4bcf75d459ed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e52d1f15e41d4a259acd70626899f74e"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"âœ… Khá»Ÿi táº¡o LLM Pipeline thÃ nh cÃ´ng!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ================================================================\n# BÆ¯á»šC 4: KHá»I Táº O RAG SYSTEM (Sá»¬A Lá»–I METADATA)\n# ================================================================\nprint(\"\\nğŸ” Äang khá»Ÿi táº¡o RAG System...\")\n\n# [CHá»ˆNH Sá»¬A] Táº¡o lá»›p Retriever an toÃ n Ä‘á»ƒ xá»­ lÃ½ metadata bá»‹ thiáº¿u\nclass SafeMetadataRetriever(BaseRetriever):\n    \"\"\"\n    Má»™t lá»›p retriever bao bá»c (wrapper) Ä‘á»ƒ Ä‘áº£m báº£o cÃ¡c tÃ i liá»‡u\n    luÃ´n cÃ³ cÃ¡c trÆ°á»ng metadata cáº§n thiáº¿t trÆ°á»›c khi Ä‘Æ°á»£c Ä‘Æ°a vÃ o prompt.\n    \"\"\"\n    base_retriever: BaseRetriever\n\n    def _get_relevant_documents(\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n    ) -> List[Document]:\n        # Láº¥y tÃ i liá»‡u tá»« retriever gá»‘c\n        docs = self.base_retriever._get_relevant_documents(query, run_manager=run_manager)\n\n        # Kiá»ƒm tra vÃ  \"vÃ¡\" metadata cho tá»«ng tÃ i liá»‡u\n        for doc in docs:\n            # Náº¿u thiáº¿u 'source_filename', láº¥y tá»« 'source' hoáº·c Ä‘áº·t giÃ¡ trá»‹ máº·c Ä‘á»‹nh\n            if \"source_filename\" not in doc.metadata:\n                doc.metadata[\"source_filename\"] = doc.metadata.get(\"source\", \"KhÃ´ng rÃµ nguá»“n\")\n            # Náº¿u thiáº¿u 'article_title', Ä‘áº·t giÃ¡ trá»‹ máº·c Ä‘á»‹nh\n            if \"article_title\" not in doc.metadata:\n                doc.metadata[\"article_title\"] = \"KhÃ´ng cÃ³ tiÃªu Ä‘á» cá»¥ thá»ƒ\"\n        return docs\n\n# 1. Load Embedding & VectorStore\nembeddings = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_PATH,\n    model_kwargs={'device': 'cuda', 'trust_remote_code':True},\n    encode_kwargs={'normalize_embeddings': True},\n)\n\nvectorstore = Chroma(\n    persist_directory=VECTOR_DB_PATH,\n    embedding_function=embeddings\n)\n\n# 2. Táº¡o retriever gá»‘c\nbase_retriever = vectorstore.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 3}\n)\n\n# 3. [CHá»ˆNH Sá»¬A] Bá»c retriever gá»‘c báº±ng SafeMetadataRetriever\nretriever = SafeMetadataRetriever(base_retriever=base_retriever)\nprint(\"âœ… ÄÃ£ Ã¡p dá»¥ng SafeMetadataRetriever Ä‘á»ƒ xá»­ lÃ½ lá»—i dá»¯ liá»‡u!\")\n\n# 4. Äá»‹nh nghÄ©a Prompt\nQA_PROMPT = PromptTemplate(\n    template=StudentRegulationPrompts.create_qa_template(),\n    input_variables=[\"context\", \"question\"]\n)\n\n# 5. Document Prompt\ndocument_prompt = PromptTemplate(\n    input_variables=[\"page_content\", \"source_filename\", \"article_title\"],\n    template=\"\"\"\n    ---------------------\n    ğŸ“„ TÃ i liá»‡u: {source_filename}\n    ğŸ”– Má»¥c: {article_title}\n    ğŸ“ Ná»™i dung: {page_content}\n    ---------------------\n    \"\"\"\n)\n\n# 6. KHá»I Táº O RETRIEVAL QA\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=retriever, # Sá»­ dá»¥ng retriever Ä‘Ã£ Ä‘Æ°á»£c \"vÃ¡\" lá»—i\n    return_source_documents=True,\n    chain_type_kwargs={\n        \"prompt\": QA_PROMPT,\n        \"document_prompt\": document_prompt\n    },\n    verbose=False\n)\n\nprint(\"âœ… RAG System SiÃªu Tá»‘c Ä‘Ã£ sáºµn sÃ ng!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:33:47.095876Z","iopub.execute_input":"2026-02-11T08:33:47.096606Z","iopub.status.idle":"2026-02-11T08:34:03.476814Z","shell.execute_reply.started":"2026-02-11T08:33:47.096579Z","shell.execute_reply":"2026-02-11T08:34:03.475979Z"}},"outputs":[{"name":"stdout","text":"\nğŸ” Äang khá»Ÿi táº¡o RAG System...\n","output_type":"stream"},{"name":"stderr","text":"WARNING:sentence_transformers.SentenceTransformer:You are trying to use a model that was created with Sentence Transformers version 5.1.2, but you're currently using version 5.1.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n/tmp/ipykernel_55/1954822006.py:37: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n  vectorstore = Chroma(\n","output_type":"stream"},{"name":"stdout","text":"âœ… ÄÃ£ Ã¡p dá»¥ng SafeMetadataRetriever Ä‘á»ƒ xá»­ lÃ½ lá»—i dá»¯ liá»‡u!\nâœ… RAG System SiÃªu Tá»‘c Ä‘Ã£ sáºµn sÃ ng!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ================================================================\n# BÆ¯á»šC 5: Xá»¬ LÃ LOGIC CHAT\n# ================================================================\n\ndef chat_with_rag(message: str) -> Tuple[str, Dict]:\n    try:\n        # Há»‡ thá»‘ng giá» Ä‘Ã¢y sáº½ khÃ´ng bá»‹ crash khi gáº·p metadata thiáº¿u\n        response = qa_chain.invoke({\"query\": message})\n\n        answer = response.get(\"result\", \"Xin lá»—i, há»‡ thá»‘ng Ä‘ang gáº·p sá»± cá»‘ vÃ  khÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i.\").strip()\n        source_docs = response.get(\"source_documents\", [])\n        sources = []\n        if source_docs:\n            for i, doc in enumerate(source_docs, 1):\n                metadata = doc.metadata\n                # Sá»­ dá»¥ng .get() vá»›i giÃ¡ trá»‹ máº·c Ä‘á»‹nh Ä‘á»ƒ an toÃ n hÆ¡n\n                full_path = metadata.get(\"source_filename\", metadata.get(\"source\", \"KhÃ´ng rÃµ\"))\n                file_name = os.path.basename(full_path)\n                article_title = metadata.get(\"article_title\", \"Ná»™i dung liÃªn quan\")\n\n                sources.append({\n                    \"STT\": i,\n                    \"TÃªn vÄƒn báº£n\": file_name,\n                    \"Chi tiáº¿t\": article_title,\n                    \"Ná»™i dung trÃ­ch dáº«n\": \" \".join(doc.page_content.split())[:300] + \"...\"\n                })\n\n        metadata_output = {\n            \"num_sources\": len(source_docs),\n            \"sources\": sources,\n            \"timestamp\": datetime.now().strftime(\"%H:%M:%S %d/%m/%Y\")\n        }\n        return answer, metadata_output\n\n    except Exception as e:\n        print(f\"Error in chat_with_rag: {str(e)}\")\n        # Tráº£ vá» má»™t thÃ´ng bÃ¡o lá»—i thÃ¢n thiá»‡n hÆ¡n cho ngÆ°á»i dÃ¹ng\n        error_message = f\"âŒ ÄÃ£ xáº£y ra lá»—i há»‡ thá»‘ng. Vui lÃ²ng thá»­ láº¡i sau.\\nChi tiáº¿t: {str(e)}\"\n        return error_message, {\"num_sources\": 0, \"sources\": []}\n\ndef clear_conversation():\n    try:\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        print(\"Conversation cleared and memory freed.\")\n        return [], \"ChÆ°a cÃ³ dá»¯ liá»‡u.\", None\n    except Exception as e:\n        print(f\"Error clearing conversation: {e}\")\n        return [], \"Lá»—i khi dá»n dáº¹p.\", None\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:34:03.477964Z","iopub.execute_input":"2026-02-11T08:34:03.478898Z","iopub.status.idle":"2026-02-11T08:34:03.486811Z","shell.execute_reply.started":"2026-02-11T08:34:03.478865Z","shell.execute_reply":"2026-02-11T08:34:03.486122Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ================================================================\n# BÆ¯á»šC 6: GIAO DIá»†N GRADIO\n# ================================================================\nprint(\"\\nğŸ¨ Äang khá»Ÿi táº¡o giao diá»‡n Gradio...\")\n\ncustom_css = \"\"\"\n#chatbot { height: 500px; overflow-y: auto; }\n#sources { height: 350px; overflow-y: auto; background-color: #f0f2f6; border-radius: 8px; padding: 10px; border: 1px solid #dee2e6; }\n\"\"\"\n\nwith gr.Blocks(css=custom_css, title=\"HUST Chatbot\", theme=gr.themes.Soft()) as demo:\n\n    gr.Markdown(\"# ğŸ“ Trá»£ lÃ½ Quy cháº¿ Sinh viÃªn HUST\")\n\n    with gr.Row():\n        with gr.Column(scale=7):\n            chatbot = gr.Chatbot(\n                elem_id=\"chatbot\",\n                label=\"Há»™i thoáº¡i\",\n                height=550,\n                avatar_images=(None, \"ğŸ¤–\"),\n                bubble_full_width=False,\n                show_copy_button=True\n            )\n            with gr.Row():\n                msg = gr.Textbox(\n                    label=\"Nháº­p cÃ¢u há»i cá»§a báº¡n táº¡i Ä‘Ã¢y...\",\n                    placeholder=\"VD: Äiá»u kiá»‡n Ä‘á»ƒ Ä‘Æ°á»£c xÃ©t há»c bá»•ng loáº¡i A lÃ  gÃ¬?\",\n                    show_label=False,\n                    lines=1,\n                    max_lines=3,\n                    scale=5,\n                    autofocus=True\n                )\n                submit_btn = gr.Button(\"Gá»­i\", variant=\"primary\", scale=1, min_width=100)\n            clear_btn = gr.Button(\"ğŸ—‘ï¸ XÃ³a há»™i thoáº¡i cÅ©\", variant=\"secondary\", size=\"sm\")\n\n        with gr.Column(scale=3):\n            with gr.Accordion(\"ğŸ’¡ HÆ°á»›ng dáº«n sá»­ dá»¥ng\", open=False):\n                gr.Markdown(\n                    \"\"\"\n                    ### CÃ¡ch sá»­ dá»¥ng chatbot:\n                    1.  **Äáº·t cÃ¢u há»i**: Nháº­p cÃ¢u há»i vá» quy cháº¿ sinh viÃªn vÃ o Ã´ bÃªn trÃ¡i.\n                    2.  **Xem pháº£n há»“i**: Chatbot sáº½ tráº£ lá»i dá»±a trÃªn vÄƒn báº£n quy cháº¿.\n                    3.  **Kiá»ƒm tra nguá»“n**: Xem cÃ¡c vÄƒn báº£n Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ cá»™t bÃªn pháº£i.\n                    ### VÃ­ dá»¥ cÃ¢u há»i:\n                    - Äiá»u kiá»‡n Ä‘á»ƒ Ä‘Æ°á»£c cáº¥p báº±ng tá»‘t nghiá»‡p lÃ  gÃ¬?\n                    - Quy Ä‘á»‹nh vá» Ä‘iá»ƒm danh trong giá» há»c ra sao?\n                    - Sinh viÃªn bá»‹ cáº£nh cÃ¡o há»c táº­p khi nÃ o?\n                    ### LÆ°u Ã½:\n                    - Chatbot Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u. Náº¿u khÃ´ng tÃ¬m tháº¥y, nÃ³ sáº½ tá»« chá»‘i.\n                    - NÃªn xÃ³a há»™i thoáº¡i cÅ© sau má»—i phiÃªn há»i-Ä‘Ã¡p Ä‘á»ƒ giáº£i phÃ³ng tÃ i nguyÃªn.\n                    \"\"\"\n                )\n            gr.Markdown(\"### ğŸ“Š Tráº¡ng thÃ¡i tÃ¬m kiáº¿m\")\n            stats_display = gr.Markdown(value=\"ChÆ°a cÃ³ dá»¯ liá»‡u.\", label=\"Thá»‘ng kÃª\")\n            gr.Markdown(\"### ğŸ“š TÃ i liá»‡u tham kháº£o\")\n            sources_display = gr.JSON(label=\"Chi tiáº¿t nguá»“n\", elem_id=\"sources\", height=400)\n\n    def user_message(user_input, chat_history):\n        if not user_input.strip():\n            return None, chat_history\n        if chat_history is None: chat_history = []\n        return \"\", chat_history + [[user_input, None]]\n\n    def bot_response(chat_history):\n        if not chat_history or not chat_history[-1][0]:\n            return chat_history, \"ChÆ°a cÃ³ dá»¯ liá»‡u.\", None\n        user_input = chat_history[-1][0]\n        bot_reply, metadata = chat_with_rag(user_input)\n        chat_history[-1][1] = bot_reply\n        stats_text = f\"**TÃ¬m tháº¥y:** {metadata['num_sources']} Ä‘oáº¡n vÄƒn báº£n liÃªn quan\"\n        if metadata['num_sources'] > 0:\n            stats_text += f\" | ğŸ•’ {metadata.get('timestamp','')}\"\n        return chat_history, stats_text, metadata[\"sources\"]\n\n    msg.submit(user_message, [msg, chatbot], [msg, chatbot], queue=False).then(\n        bot_response, chatbot, [chatbot, stats_display, sources_display]\n    )\n    submit_btn.click(user_message, [msg, chatbot], [msg, chatbot], queue=False).then(\n        bot_response, chatbot, [chatbot, stats_display, sources_display]\n    )\n    clear_btn.click(clear_conversation, None, [chatbot, stats_display, sources_display], queue=False)\n\nprint(\"\\nğŸŒ Khá»Ÿi Ä‘á»™ng Gradio...\")\ndemo.queue().launch(share=True, debug=True, server_name=\"0.0.0.0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:34:03.487689Z","iopub.execute_input":"2026-02-11T08:34:03.488024Z"}},"outputs":[{"name":"stdout","text":"\nğŸ¨ Äang khá»Ÿi táº¡o giao diá»‡n Gradio...\n\nğŸŒ Khá»Ÿi Ä‘á»™ng Gradio...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/1695057564.py:17: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n  chatbot = gr.Chatbot(\n/tmp/ipykernel_55/1695057564.py:17: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n  chatbot = gr.Chatbot(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://0.0.0.0:7860\n* Running on public URL: https://ec00cd8bc22a7fe3bd.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://ec00cd8bc22a7fe3bd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":null}]}
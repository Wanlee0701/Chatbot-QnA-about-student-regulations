{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmuGJxPfqjvA",
        "outputId": "03a45e82-46dd-443b-9e20-0585abafb10b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-community langchain-huggingface chromadb langchain-core\n",
        "!pip install -q sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "RPrrcmwTrujk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import shutil"
      ],
      "metadata": {
        "id": "GpRR0OJguIfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRXOeZdDsWhI",
        "outputId": "0573f40f-6fa2-42fb-9e27-e03d6888b1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core import vectorstores\n",
        "data_cleaned_dir = \"/content/drive/MyDrive/chatbot_project/data_cleaned\"\n",
        "embedding_model_path = \"/content/drive/MyDrive/chatbot_project/models/embedding\"\n",
        "vector_db_dir = \"/content/drive/MyDrive/chatbot_project/vectorstore/chroma_db\"\n"
      ],
      "metadata": {
        "id": "A16NgA8bs9K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Láº¥y mÃ£ nguá»“n tá»« file \"ETL\" Ä‘á»ƒ thá»±c hiá»‡n táº¡o cÃ¡c chunk"
      ],
      "metadata": {
        "id": "S0ZHmeKrtr-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Äá»‹nh nghÄ©a cáº¥u hÃ¬nh cáº¯t\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"chapter_title\"),  # Cáº¥p 1: ChÆ°Æ¡ng\n",
        "    (\"##\", \"article_title\"), # Cáº¥p 2: Äiá»u\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "# Bá»™ cáº¯t phá»¥: Chá»‰ dÃ¹ng khi 1 Äiá»u quÃ¡ dÃ i (VD: Äiá»u 12 dÃ i > 2000 kÃ½ tá»±)\n",
        "text_recursive = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "def split_text_keeping_tables(text):\n",
        "    # Regex tÃ¬m báº£ng Markdown:\n",
        "    # Báº¯t Ä‘áº§u báº±ng |...|, dÃ²ng tiáº¿p theo |---|...|, vÃ  cÃ¡c dÃ²ng tiáº¿p theo báº¯t Ä‘áº§u báº±ng |\n",
        "    # (?m) báº­t cháº¿ Ä‘á»™ multiline\n",
        "    table_pattern = r'(?:\\|.*\\|\\n\\|[-:| ]+\\|\\n(?:\\|.*\\|\\n?)*)'\n",
        "\n",
        "    # TÃ¬m táº¥t cáº£ cÃ¡c báº£ng\n",
        "    tables = list(re.finditer(table_pattern, text))\n",
        "\n",
        "    if not tables:\n",
        "        # Náº¿u khÃ´ng cÃ³ báº£ng, cáº¯t bÃ¬nh thÆ°á»ng\n",
        "        return text_recursive.split_text(text)\n",
        "\n",
        "    final_chunks = []\n",
        "    last_end = 0\n",
        "\n",
        "    for match in tables:\n",
        "        start, end = match.span()\n",
        "\n",
        "        # 1. Xá»¬ LÃ PHáº¦N VÄ‚N Báº¢N TRÆ¯á»šC Báº¢NG\n",
        "        pre_table_text = text[last_end:start]\n",
        "        if pre_table_text.strip():\n",
        "            final_chunks.extend(text_recursive.split_text(pre_table_text))\n",
        "\n",
        "        # 2. Xá»¬ LÃ PHáº¦N Báº¢NG (GIá»® NGUYÃŠN)\n",
        "        table_text = match.group()\n",
        "        if len(table_text) > 4000:\n",
        "             final_chunks.append(table_text)\n",
        "        else:\n",
        "             final_chunks.append(table_text)\n",
        "        last_end = end\n",
        "\n",
        "    # 3. Xá»¬ LÃ PHáº¦N VÄ‚N Báº¢N CÃ’N Láº I SAU Báº¢NG CUá»I\n",
        "    post_table_text = text[last_end:]\n",
        "    if post_table_text.strip():\n",
        "        final_chunks.extend(text_recursive.split_text(post_table_text))\n",
        "\n",
        "    return final_chunks"
      ],
      "metadata": {
        "id": "ON_lhRWqtmRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_json_file(json_path):\n",
        "# Äá»c file JSON\n",
        "  with open(json_path, 'r', encoding='utf-8') as f:\n",
        "      data = json.load(f)\n",
        "\n",
        "  # Láº¥y Metadata gá»‘c (Cá»±c ká»³ quan trá»ng)\n",
        "  base_metadata = data['metadata']\n",
        "  content = data['content']\n",
        "\n",
        "  # BÆ¯á»šC 1: Cáº¯t theo cáº¥u trÃºc Markdown (ChÆ°Æ¡ng/Äiá»u)\n",
        "  md_header_splits = markdown_splitter.split_text(content)\n",
        "  final_chunks = []\n",
        "\n",
        "  # BÆ¯á»šC 2: Kiá»ƒm tra Ä‘á»™ dÃ i vÃ  Gáº¯n Metadata gá»‘c\n",
        "  for split in md_header_splits:\n",
        "    content_of_article = split.page_content\n",
        "    metadata_of_article = split.metadata\n",
        "    sub_chunks = split_text_keeping_tables(content_of_article)\n",
        "    for chunk_text in sub_chunks:\n",
        "          # Kiá»ƒm tra xem chunk nÃ y cÃ³ pháº£i lÃ  báº£ng khÃ´ng Ä‘á»ƒ gÃ¡n nhÃ£n (Optional)\n",
        "          is_table = chunk_text.strip().startswith(\"|\")\n",
        "\n",
        "          combined_metadata = {\n",
        "              **base_metadata,\n",
        "              **metadata_of_article,\n",
        "              \"is_table\": is_table # ThÃªm cá» nÃ y Ä‘á»ƒ sau nÃ y dá»… lá»c\n",
        "          }\n",
        "\n",
        "          doc = Document(\n",
        "              page_content=chunk_text,\n",
        "              metadata=combined_metadata\n",
        "          )\n",
        "          final_chunks.append(doc)\n",
        "  return final_chunks"
      ],
      "metadata": {
        "id": "uoBLlp2BuY5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [f for f in os.listdir(data_cleaned_dir) if f.endswith('.json')]\n",
        "all_chunk = []\n",
        "for filename in files:\n",
        "    file_path = os.path.join(data_cleaned_dir, filename)\n",
        "    print(f\"Processing: {filename}\")\n",
        "    file_docs = chunk_json_file(file_path)\n",
        "    all_chunk.extend(file_docs)\n",
        "print(f\"FINISHED with {len(all_chunk)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgHmcjk5t9ms",
        "outputId": "0592d633-e056-47b6-e6da-ca945ddb62c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: Quy_che_25.json\n",
            "Processing: QD_NN_K68.json\n",
            "Processing: HD_hoc_chuyen_tiep_ky_su_180TC.json\n",
            "Processing: QD_ban_hanh_QD_to_chuc_thi_Truc_tuyen.json\n",
            "Processing: QD_chuyen_doi_hoc_phan_tuong_duong.json\n",
            "Processing: Quy_che_CTSV_ÄHBK_HN_2025310_final.json\n",
            "Processing: Hoc_bong_KKHT_2023.json\n",
            "Processing: Hoc_bong_TDN_2023.json\n",
            "Processing: QD_NN_K70.json\n",
            "Processing: QD_NN_K65.json\n",
            "Processing: QD_ban_hanh_QD_to_chuc_day_hoc_tren_nen_tang_CN_ket_noi.json\n",
            "FINISHED with 489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Thá»±c hiá»‡n Embedding"
      ],
      "metadata": {
        "id": "3jmLhNrgutIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_db():\n",
        "    # 1. Chuáº©n bá»‹ Documents\n",
        "    documents = all_chunk\n",
        "\n",
        "    if not documents:\n",
        "        print(\"KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½.\")\n",
        "        return\n",
        "    print(f\"\\nÄang táº£i model embedding: {embedding_model_path}...\")\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=embedding_model_path,\n",
        "        model_kwargs={'device': 'cuda', 'trust_remote_code': True}, # DÃ¹ng GPU náº¿u cÃ³\n",
        "        encode_kwargs={'normalize_embeddings': True} # GTE cáº§n normalize\n",
        "    )\n",
        "\n",
        "    print(f\"Äang táº¡o Vector Database táº¡i: {vector_db_dir}\")\n",
        "\n",
        "    # XÃ³a DB cÅ© náº¿u muá»‘n lÃ m má»›i hoÃ n toÃ n (Cáº©n tháº­n!)\n",
        "    if os.path.exists(vector_db_dir):\n",
        "        shutil.rmtree(vector_db_dir)\n",
        "\n",
        "    # Táº¡o DB vÃ  náº¡p dá»¯ liá»‡u (Batch processing tá»± Ä‘á»™ng)\n",
        "    # Chroma tá»± Ä‘á»™ng lÆ°u xuá»‘ng á»• cá»©ng\n",
        "    vector_db = Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embedding_model,\n",
        "        persist_directory=vector_db_dir\n",
        "    )\n",
        "\n",
        "    print(\"âœ… ÄÃ£ lÆ°u Vector Database thÃ nh cÃ´ng!\")\n",
        "    return vector_db"
      ],
      "metadata": {
        "id": "J2m-VhltuxCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = create_vector_db()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md7ooFPs3fvl",
        "outputId": "9a89c239-ebd1-4634-f941-5d3b7235223d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Äang táº£i model embedding: /content/drive/MyDrive/chatbot_project/models/embedding...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/chatbot_project/models/embedding' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Äang táº¡o Vector Database táº¡i: /content/drive/MyDrive/chatbot_project/vectorstore/chroma_db\n",
            "âœ… ÄÃ£ lÆ°u Vector Database thÃ nh cÃ´ng!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- [TEST TRUY Váº¤N Vá»šI ÄIá»‚M Sá»] ---\")\n",
        "query = \"Äiá»u kiá»‡n nháº­n há»c bá»•ng loáº¡i A\"\n",
        "\n",
        "# Sá»­ dá»¥ng similarity_search_with_score thay vÃ¬ similarity_search thÆ°á»ng\n",
        "# Káº¿t quáº£ tráº£ vá» lÃ  má»™t list cÃ¡c tuple: (Document, Score)\n",
        "results_with_score = db.similarity_search_with_score(query, k=3)\n",
        "\n",
        "for i, (doc, score) in enumerate(results_with_score):\n",
        "    similarity_percent = (1 - score) * 100\n",
        "    if similarity_percent < 0: similarity_percent = 0\n",
        "\n",
        "    print(f\"\\nâœ… Káº¾T QUáº¢ {i+1}:\")\n",
        "    print(f\"   ğŸ“„ Nguá»“n: {doc.metadata.get('source_filename')} - {doc.metadata.get('article_title')}\")\n",
        "    print(f\"   ğŸ“ Khoáº£ng cÃ¡ch (Distance): {score:.4f} (CÃ ng nhá» cÃ ng tá»‘t)\")\n",
        "    print(f\"   ğŸ¯ Äá»™ tÆ°Æ¡ng Ä‘á»“ng Æ°á»›c lÆ°á»£ng: {similarity_percent:.2f}%\")\n",
        "    print(f\"   ğŸ“ Ná»™i dung trÃ­ch dáº«n: \\\"{doc.page_content[:200]}...\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPoSGPxvzGBT",
        "outputId": "78edb1f5-1fab-4a30-9439-46da9d49bc23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- [TEST TRUY Váº¤N Vá»šI ÄIá»‚M Sá»] ---\n",
            "\n",
            "âœ… Káº¾T QUáº¢ 1:\n",
            "   ğŸ“„ Nguá»“n: Hoc_bong_KKHT_2023.pdf - Äiá»u 3. TiÃªu chuáº©n xÃ©t cáº¥p há»c bá»•ng\n",
            "   ğŸ“ Khoáº£ng cÃ¡ch (Distance): 0.4699 (CÃ ng nhá» cÃ ng tá»‘t)\n",
            "   ğŸ¯ Äá»™ tÆ°Æ¡ng Ä‘á»“ng Æ°á»›c lÆ°á»£ng: 53.01%\n",
            "   ğŸ“ Ná»™i dung trÃ­ch dáº«n: \"1. Há»c bá»•ng loáº¡i C: GPA â‰¥ 2,5 vÃ  Ä‘iá»ƒm rÃ¨n luyá»‡n há»c ká»³ â‰¥ 65 Ä‘iá»ƒm.\n",
            "2. Há»c bá»•ng loáº¡i B: GPA â‰¥ 3,2 vÃ  Ä‘iá»ƒm rÃ¨n luyá»‡n há»c ká»³ â‰¥ 80 Ä‘iá»ƒm.\n",
            "3. Há»c bá»•ng loáº¡i A: GPA â‰¥ 3,6 vÃ  Ä‘iá»ƒm rÃ¨n luyá»‡n há»c ká»³ â‰¥ 90 Ä‘iá»ƒm....\"\n",
            "\n",
            "âœ… Káº¾T QUáº¢ 2:\n",
            "   ğŸ“„ Nguá»“n: Hoc_bong_TDN_2023.pdf - Äiá»u 6. NguyÃªn táº¯c xÃ©t, cáº¥p Há»c bá»•ng Tráº§n Äáº¡i NghÄ©a\n",
            "   ğŸ“ Khoáº£ng cÃ¡ch (Distance): 0.4901 (CÃ ng nhá» cÃ ng tá»‘t)\n",
            "   ğŸ¯ Äá»™ tÆ°Æ¡ng Ä‘á»“ng Æ°á»›c lÆ°á»£ng: 50.99%\n",
            "   ğŸ“ Ná»™i dung trÃ­ch dáº«n: \"1. Há»c bá»•ng Tráº§n Äáº¡i NghÄ©a Ä‘Æ°á»£c xÃ©t, cáº¥p theo há»c ká»³ Ä‘á»‘i vá»›i cÃ¡c trÆ°á»ng há»£p\n",
            "quy Ä‘á»‹nh táº¡i khoáº£n 1 vÃ  khoáº£n 2 Äiá»u 5; xÃ©t cáº¥p thÆ°á»ng xuyÃªn Ä‘á»‘i vá»›i cÃ¡c trÆ°á»ng há»£p\n",
            "quy Ä‘á»‹nh táº¡i khoáº£n 3 vÃ  khoáº£n 4 Äiá»u 5.\n",
            "...\"\n",
            "\n",
            "âœ… Káº¾T QUáº¢ 3:\n",
            "   ğŸ“„ Nguá»“n: Hoc_bong_KKHT_2023.pdf - Äiá»u 4. NguyÃªn táº¯c xÃ©t cáº¥p há»c bá»•ng\n",
            "   ğŸ“ Khoáº£ng cÃ¡ch (Distance): 0.5034 (CÃ ng nhá» cÃ ng tá»‘t)\n",
            "   ğŸ¯ Äá»™ tÆ°Æ¡ng Ä‘á»“ng Æ°á»›c lÆ°á»£ng: 49.66%\n",
            "   ğŸ“ Ná»™i dung trÃ­ch dáº«n: \"1. Sá»­ dá»¥ng káº¿t quáº£ há»c táº­p vÃ  rÃ¨n luyá»‡n cá»§a há»c ká»³ liá»n trÆ°á»›c Ä‘á»ƒ xÃ©t cáº¥p há»c\n",
            "bá»•ng cho há»c ká»³ sau.\n",
            "2. Há»c bá»•ng Ä‘Æ°á»£c xÃ©t theo khÃ³a, ngÃ nh Ä‘Ã o táº¡o vÃ  theo thá»© tá»± Æ°u tiÃªn láº§n lÆ°á»£t tá»«\n",
            "loáº¡i A Ä‘áº¿n loáº¡i C, tá»«...\"\n"
          ]
        }
      ]
    }
  ]
}